{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3927714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac18841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 25\n",
    "lr = 0.01\n",
    "epochs= 1000\n",
    "\n",
    "string = \"hello pytorch. how long can a rnn cell remember? show me your limit!\"\n",
    "chars = \"abcdefghijklmnopqrstuvwxyz ?!.,:;01\"\n",
    "char_list = [i for i in chars]\n",
    "n_letters = len(char_list)\n",
    "print(n_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1287b912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장이 들어왔을때, 시작토큰과 끝 토큰을 붙이고 문자하나씩 원핫벡터로 변환하는 함수\n",
    "def string_to_onehot(string):\n",
    "    start = np.zeros(shape=len(char_list), dtype=int)\n",
    "    end = np.zeros(shape=len(char_list), dtype=int)\n",
    "    start[-2] = 1\n",
    "    end[-1] = 1\n",
    "    for i in string:\n",
    "        idx = char_list.index(i)\n",
    "        zero = np.zeros(shape= n_letters, dtype=int)\n",
    "        zero[idx] =1\n",
    "        start = np.vstack([start, zero])\n",
    "    output = np.vstack([start, end])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ed346d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "output = string_to_onehot('abc')\n",
    "print(output)#잘 출력되는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38a50e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#원핫벡터(벡터하나)를 다시 문자로 바꾸는 함수\n",
    "def onehot_to_word(onehot_1):\n",
    "    return char_list[onehot_1.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7203b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(output, dtype=torch.int64))\n",
    "print(onehot_to_word(torch.tensor(output)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff73a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple RNN\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN,self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.act_fn = nn.Tanh()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        hidden = self.act_fn(self.i2h(input)+self.h2h(hidden))\n",
    "        output = self.h2o(hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "rnn = RNN(n_letters, n_hidden, n_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d33e57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ea3382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8392, grad_fn=<AddBackward0>)\n",
      "tensor(1.6709, grad_fn=<AddBackward0>)\n",
      "tensor(1.3423, grad_fn=<AddBackward0>)\n",
      "tensor(1.1048, grad_fn=<AddBackward0>)\n",
      "tensor(0.9319, grad_fn=<AddBackward0>)\n",
      "tensor(0.7785, grad_fn=<AddBackward0>)\n",
      "tensor(0.7574, grad_fn=<AddBackward0>)\n",
      "tensor(0.6042, grad_fn=<AddBackward0>)\n",
      "tensor(0.5602, grad_fn=<AddBackward0>)\n",
      "tensor(0.4887, grad_fn=<AddBackward0>)\n",
      "tensor(0.4497, grad_fn=<AddBackward0>)\n",
      "tensor(0.4411, grad_fn=<AddBackward0>)\n",
      "tensor(0.3991, grad_fn=<AddBackward0>)\n",
      "tensor(0.3726, grad_fn=<AddBackward0>)\n",
      "tensor(0.3509, grad_fn=<AddBackward0>)\n",
      "tensor(0.3351, grad_fn=<AddBackward0>)\n",
      "tensor(0.3232, grad_fn=<AddBackward0>)\n",
      "tensor(0.3078, grad_fn=<AddBackward0>)\n",
      "tensor(0.2994, grad_fn=<AddBackward0>)\n",
      "tensor(0.3011, grad_fn=<AddBackward0>)\n",
      "tensor(0.3079, grad_fn=<AddBackward0>)\n",
      "tensor(0.2825, grad_fn=<AddBackward0>)\n",
      "tensor(0.3080, grad_fn=<AddBackward0>)\n",
      "tensor(0.2757, grad_fn=<AddBackward0>)\n",
      "tensor(0.2628, grad_fn=<AddBackward0>)\n",
      "tensor(0.3022, grad_fn=<AddBackward0>)\n",
      "tensor(0.2595, grad_fn=<AddBackward0>)\n",
      "tensor(0.2491, grad_fn=<AddBackward0>)\n",
      "tensor(0.3456, grad_fn=<AddBackward0>)\n",
      "tensor(0.3456, grad_fn=<AddBackward0>)\n",
      "tensor(0.2910, grad_fn=<AddBackward0>)\n",
      "tensor(0.2691, grad_fn=<AddBackward0>)\n",
      "tensor(0.2559, grad_fn=<AddBackward0>)\n",
      "tensor(0.2499, grad_fn=<AddBackward0>)\n",
      "tensor(0.2561, grad_fn=<AddBackward0>)\n",
      "tensor(0.2455, grad_fn=<AddBackward0>)\n",
      "tensor(0.2392, grad_fn=<AddBackward0>)\n",
      "tensor(0.2774, grad_fn=<AddBackward0>)\n",
      "tensor(0.2359, grad_fn=<AddBackward0>)\n",
      "tensor(0.2284, grad_fn=<AddBackward0>)\n",
      "tensor(0.2386, grad_fn=<AddBackward0>)\n",
      "tensor(0.2284, grad_fn=<AddBackward0>)\n",
      "tensor(0.2226, grad_fn=<AddBackward0>)\n",
      "tensor(0.2312, grad_fn=<AddBackward0>)\n",
      "tensor(0.2201, grad_fn=<AddBackward0>)\n",
      "tensor(0.2143, grad_fn=<AddBackward0>)\n",
      "tensor(0.2213, grad_fn=<AddBackward0>)\n",
      "tensor(0.2171, grad_fn=<AddBackward0>)\n",
      "tensor(0.2096, grad_fn=<AddBackward0>)\n",
      "tensor(0.2140, grad_fn=<AddBackward0>)\n",
      "tensor(0.2219, grad_fn=<AddBackward0>)\n",
      "tensor(0.2048, grad_fn=<AddBackward0>)\n",
      "tensor(0.2000, grad_fn=<AddBackward0>)\n",
      "tensor(0.1983, grad_fn=<AddBackward0>)\n",
      "tensor(0.2126, grad_fn=<AddBackward0>)\n",
      "tensor(0.1980, grad_fn=<AddBackward0>)\n",
      "tensor(0.1929, grad_fn=<AddBackward0>)\n",
      "tensor(0.3125, grad_fn=<AddBackward0>)\n",
      "tensor(0.2047, grad_fn=<AddBackward0>)\n",
      "tensor(0.1955, grad_fn=<AddBackward0>)\n",
      "tensor(0.1905, grad_fn=<AddBackward0>)\n",
      "tensor(0.1868, grad_fn=<AddBackward0>)\n",
      "tensor(0.2562, grad_fn=<AddBackward0>)\n",
      "tensor(0.2107, grad_fn=<AddBackward0>)\n",
      "tensor(0.1968, grad_fn=<AddBackward0>)\n",
      "tensor(0.1903, grad_fn=<AddBackward0>)\n",
      "tensor(0.1908, grad_fn=<AddBackward0>)\n",
      "tensor(0.1855, grad_fn=<AddBackward0>)\n",
      "tensor(0.1820, grad_fn=<AddBackward0>)\n",
      "tensor(0.1851, grad_fn=<AddBackward0>)\n",
      "tensor(0.1901, grad_fn=<AddBackward0>)\n",
      "tensor(0.1817, grad_fn=<AddBackward0>)\n",
      "tensor(0.1786, grad_fn=<AddBackward0>)\n",
      "tensor(0.1754, grad_fn=<AddBackward0>)\n",
      "tensor(0.1725, grad_fn=<AddBackward0>)\n",
      "tensor(0.2001, grad_fn=<AddBackward0>)\n",
      "tensor(0.1979, grad_fn=<AddBackward0>)\n",
      "tensor(0.2222, grad_fn=<AddBackward0>)\n",
      "tensor(0.1867, grad_fn=<AddBackward0>)\n",
      "tensor(0.1798, grad_fn=<AddBackward0>)\n",
      "tensor(0.1753, grad_fn=<AddBackward0>)\n",
      "tensor(0.1724, grad_fn=<AddBackward0>)\n",
      "tensor(0.2207, grad_fn=<AddBackward0>)\n",
      "tensor(0.1853, grad_fn=<AddBackward0>)\n",
      "tensor(0.1773, grad_fn=<AddBackward0>)\n",
      "tensor(0.1733, grad_fn=<AddBackward0>)\n",
      "tensor(0.1765, grad_fn=<AddBackward0>)\n",
      "tensor(0.1705, grad_fn=<AddBackward0>)\n",
      "tensor(0.1671, grad_fn=<AddBackward0>)\n",
      "tensor(0.1651, grad_fn=<AddBackward0>)\n",
      "tensor(0.2258, grad_fn=<AddBackward0>)\n",
      "tensor(0.1680, grad_fn=<AddBackward0>)\n",
      "tensor(0.1643, grad_fn=<AddBackward0>)\n",
      "tensor(0.1624, grad_fn=<AddBackward0>)\n",
      "tensor(0.1807, grad_fn=<AddBackward0>)\n",
      "tensor(0.1648, grad_fn=<AddBackward0>)\n",
      "tensor(0.1603, grad_fn=<AddBackward0>)\n",
      "tensor(0.1571, grad_fn=<AddBackward0>)\n",
      "tensor(0.1557, grad_fn=<AddBackward0>)\n",
      "tensor(0.1961, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "one_hot = torch.from_numpy(string_to_onehot(string)).type_as(torch.FloatTensor())\n",
    "\n",
    "for i in range(epochs):\n",
    "    rnn.zero_grad()\n",
    "    total_loss = 0\n",
    "    hidden = rnn.init_hidden()\n",
    "    \n",
    "    for j in range(one_hot.size()[0]-1):\n",
    "        input_ = one_hot[j]  #input_shape = (n_letters)\n",
    "        target = one_hot[j+1]#target_shape = (n_letters)\n",
    "        \n",
    "        output, hidden = rnn.forward(input_, hidden) #output_shape = (1, n_letters)\n",
    "        loss = loss_func(output.view(-1),target)\n",
    "        total_loss += loss\n",
    "    \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i%10 == 0:\n",
    "        print(total_loss)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3de2bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hellcmeite teiteieeieciectecteiteiteieciectecteiteiteieeiectectectei\n"
     ]
    }
   ],
   "source": [
    "start = torch.zeros(1,len(char_list))\n",
    "start[:, -2] = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    hidden = rnn.init_hidden()\n",
    "    input_ = start\n",
    "    output_string = \"\"\n",
    "    for i in range(len(string)):\n",
    "        output, hidden = rnn.forward(input_, hidden)\n",
    "        output_string += onehot_to_word(output)\n",
    "        input_ = output\n",
    "        \n",
    "print(output_string)  #뒤로 갈수록 잘 학습하지 못한다.      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87da794d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 5]) torch.Size([2, 3, 5]) torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "rnn = nn.LSTM(input_size=3, hidden_size=5, num_layers=2,batch_first=True)#num_layers는 lstm을 몇 층으로 쌓을 것인지 나타냄\n",
    "# 입력의 형태는 (seq_len, batch, input_size), batch_first=True라 하면, batch가 가장 먼저 옴.\n",
    "input_ = torch.randn(3, 5, 3)#여기서는 (batch, seq_len, input_size)\n",
    "# hidden state, cell state의 형태는 (num_layers * num_directions, batch, hidden_size)\n",
    "h0 = torch.randn(2, 3, 5)\n",
    "c0 = torch.randn(2, 3, 5)\n",
    "# LSTM에 입력을 전달할때는 input, (h_0, c_0) 처럼 상태를 튜플로 묶어서 전달.\n",
    "output, (hidden_state, cell_state) = rnn(input_, (h0, c0))\n",
    "print(output.size(),hidden_state.size(),cell_state.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c53eba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 35])\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "# 이번코드는 배치사이즈가 1보다 큰 경우에 대해 만들었습니다.\n",
    "batch_size = 5\n",
    "\n",
    "# seq_len는 바꿔도 학습은 되지만 테스트시 편의성을 위해 1로 설정했습니다.\n",
    "seq_len = 1\n",
    "\n",
    "# num_layers는 자유롭게 바꿀 수 있습니다.\n",
    "num_layers = 3\n",
    "input_size = n_letters #35\n",
    "hidden_size = 35\n",
    "lr = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "one_hot = torch.from_numpy(string_to_onehot(string)).type_as(torch.FloatTensor())\n",
    "\n",
    "print(one_hot.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9eacbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self,input,hidden,cell):\n",
    "        output,(hidden,cell) = self.lstm(input,(hidden,cell))\n",
    "        \n",
    "        return output,hidden,cell\n",
    "    \n",
    "    def init_hidden_cell(self, batch_size):\n",
    "        hidden = torch.zeros(num_layers, batch_size, hidden_size) \n",
    "        cell = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "        return hidden,cell\n",
    "    \n",
    "lstm = RNN(input_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff0d8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function & Optimizer\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6123342c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 35])\n",
      "torch.Size([3, 5, 35]) torch.Size([3, 5, 35])\n",
      "torch.Size([5, 1, 35]) torch.Size([3, 5, 35]) torch.Size([3, 5, 35])\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "input_data = one_hot[j:j+batch_size].view(batch_size,seq_len,input_size)\n",
    "print(input_data.size())\n",
    "\n",
    "hidden,cell = lstm.init_hidden_cell(batch_size)\n",
    "print(hidden.size(),cell.size())\n",
    "\n",
    "output,hidden,cell = lstm(input_data,hidden,cell)\n",
    "print(output.size(),hidden.size(),cell.size())\n",
    "#output = (batch, seq_len, hidden_size)\n",
    "#hidden, cell = (num_layer*num_direction, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68f74c68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1304, grad_fn=<AddBackward0>)\n",
      "tensor(1.6977, grad_fn=<AddBackward0>)\n",
      "tensor(1.6031, grad_fn=<AddBackward0>)\n",
      "tensor(1.4696, grad_fn=<AddBackward0>)\n",
      "tensor(1.2395, grad_fn=<AddBackward0>)\n",
      "tensor(0.8986, grad_fn=<AddBackward0>)\n",
      "tensor(0.5138, grad_fn=<AddBackward0>)\n",
      "tensor(0.2935, grad_fn=<AddBackward0>)\n",
      "tensor(0.1465, grad_fn=<AddBackward0>)\n",
      "tensor(0.0893, grad_fn=<AddBackward0>)\n",
      "tensor(0.0627, grad_fn=<AddBackward0>)\n",
      "tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(0.0390, grad_fn=<AddBackward0>)\n",
      "tensor(0.0323, grad_fn=<AddBackward0>)\n",
      "tensor(0.0276, grad_fn=<AddBackward0>)\n",
      "tensor(0.0242, grad_fn=<AddBackward0>)\n",
      "tensor(0.0216, grad_fn=<AddBackward0>)\n",
      "tensor(0.0189, grad_fn=<AddBackward0>)\n",
      "tensor(0.0170, grad_fn=<AddBackward0>)\n",
      "tensor(0.0163, grad_fn=<AddBackward0>)\n",
      "tensor(0.0145, grad_fn=<AddBackward0>)\n",
      "tensor(0.0131, grad_fn=<AddBackward0>)\n",
      "tensor(0.0117, grad_fn=<AddBackward0>)\n",
      "tensor(0.0110, grad_fn=<AddBackward0>)\n",
      "tensor(0.0106, grad_fn=<AddBackward0>)\n",
      "tensor(0.0103, grad_fn=<AddBackward0>)\n",
      "tensor(0.0099, grad_fn=<AddBackward0>)\n",
      "tensor(0.0097, grad_fn=<AddBackward0>)\n",
      "tensor(0.0095, grad_fn=<AddBackward0>)\n",
      "tensor(0.0093, grad_fn=<AddBackward0>)\n",
      "tensor(0.0091, grad_fn=<AddBackward0>)\n",
      "tensor(0.0090, grad_fn=<AddBackward0>)\n",
      "tensor(0.0089, grad_fn=<AddBackward0>)\n",
      "tensor(0.0088, grad_fn=<AddBackward0>)\n",
      "tensor(0.0087, grad_fn=<AddBackward0>)\n",
      "tensor(0.0086, grad_fn=<AddBackward0>)\n",
      "tensor(0.0085, grad_fn=<AddBackward0>)\n",
      "tensor(0.0089, grad_fn=<AddBackward0>)\n",
      "tensor(0.0085, grad_fn=<AddBackward0>)\n",
      "tensor(0.0083, grad_fn=<AddBackward0>)\n",
      "tensor(0.0082, grad_fn=<AddBackward0>)\n",
      "tensor(0.0082, grad_fn=<AddBackward0>)\n",
      "tensor(0.0081, grad_fn=<AddBackward0>)\n",
      "tensor(0.0080, grad_fn=<AddBackward0>)\n",
      "tensor(0.0080, grad_fn=<AddBackward0>)\n",
      "tensor(0.0080, grad_fn=<AddBackward0>)\n",
      "tensor(0.0079, grad_fn=<AddBackward0>)\n",
      "tensor(0.0079, grad_fn=<AddBackward0>)\n",
      "tensor(0.0078, grad_fn=<AddBackward0>)\n",
      "tensor(0.0078, grad_fn=<AddBackward0>)\n",
      "tensor(0.0078, grad_fn=<AddBackward0>)\n",
      "tensor(0.0077, grad_fn=<AddBackward0>)\n",
      "tensor(0.0077, grad_fn=<AddBackward0>)\n",
      "tensor(0.0077, grad_fn=<AddBackward0>)\n",
      "tensor(0.0077, grad_fn=<AddBackward0>)\n",
      "tensor(0.0077, grad_fn=<AddBackward0>)\n",
      "tensor(0.0077, grad_fn=<AddBackward0>)\n",
      "tensor(0.0076, grad_fn=<AddBackward0>)\n",
      "tensor(0.0076, grad_fn=<AddBackward0>)\n",
      "tensor(0.0076, grad_fn=<AddBackward0>)\n",
      "tensor(0.0075, grad_fn=<AddBackward0>)\n",
      "tensor(0.0075, grad_fn=<AddBackward0>)\n",
      "tensor(0.0075, grad_fn=<AddBackward0>)\n",
      "tensor(0.0075, grad_fn=<AddBackward0>)\n",
      "tensor(0.0075, grad_fn=<AddBackward0>)\n",
      "tensor(0.0074, grad_fn=<AddBackward0>)\n",
      "tensor(0.0074, grad_fn=<AddBackward0>)\n",
      "tensor(0.0074, grad_fn=<AddBackward0>)\n",
      "tensor(0.0074, grad_fn=<AddBackward0>)\n",
      "tensor(0.0075, grad_fn=<AddBackward0>)\n",
      "tensor(0.0074, grad_fn=<AddBackward0>)\n",
      "tensor(0.0074, grad_fn=<AddBackward0>)\n",
      "tensor(0.0073, grad_fn=<AddBackward0>)\n",
      "tensor(0.0073, grad_fn=<AddBackward0>)\n",
      "tensor(0.0073, grad_fn=<AddBackward0>)\n",
      "tensor(0.0073, grad_fn=<AddBackward0>)\n",
      "tensor(0.0073, grad_fn=<AddBackward0>)\n",
      "tensor(0.0073, grad_fn=<AddBackward0>)\n",
      "tensor(0.0073, grad_fn=<AddBackward0>)\n",
      "tensor(0.0073, grad_fn=<AddBackward0>)\n",
      "tensor(0.0073, grad_fn=<AddBackward0>)\n",
      "tensor(0.0073, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0076, grad_fn=<AddBackward0>)\n",
      "tensor(0.0073, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "unroll_len = one_hot.size()[0]//seq_len -1#75//1 -1 = 74 끝 토큰은 입력으로 훈련시킬 필요없으므로\n",
    "for i in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    hidden,cell = lstm.init_hidden_cell(batch_size)\n",
    "    \n",
    "    loss = 0\n",
    "    for j in range(unroll_len-batch_size+1):#74-3+1= 72-> index 71,72,73 벡터를 batch로 쌓는다.\n",
    "        \n",
    "        # batch size에 맞게 one-hot 벡터를 스택 합니다.\n",
    "        # 예를 들어 batch size가 3이면 pytorch에서 pyt를 one-hot 벡터로 바꿔서 쌓고\n",
    "        # 목표값으로 yto를 one-hot 벡터로 바꿔서 쌓는 과정입니다.\n",
    "        input_data = torch.stack([one_hot[j+k:j+k+seq_len] for k in range(batch_size)],dim=0)\n",
    "        #input_data = (batch, seq_len, input_size) = ()\n",
    "        label = torch.stack([one_hot[j+k+1:j+k+seq_len+1] for k in range(batch_size)],dim=0)\n",
    "        #output_data = (batch,, seq_len, hidden_size)\n",
    "        input_data = input_data\n",
    "        label = label\n",
    "        \n",
    "        output, hidden, cell = lstm(input_data,hidden,cell)\n",
    "        loss += loss_func(output.view(1,-1),label.view(1,-1))\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd8f37db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello pytorch. how long can a rnn cell remember? show me your limit!\n"
     ]
    }
   ],
   "source": [
    "start = torch.zeros(1,len(char_list))\n",
    "start[:, -2] = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    hidden,cell = lstm.init_hidden_cell(1)\n",
    "    input_ = start.reshape(1,seq_len,input_size)\n",
    "    output_string = \"\"\n",
    "    for i in range(len(string)):\n",
    "        output,hidden,cell = lstm(input_,hidden,cell)\n",
    "        output_string += onehot_to_word(output.view(-1))\n",
    "        input_ = output\n",
    "        \n",
    "print(output_string) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9276d838",
   "metadata": {},
   "source": [
    "## embedding\n",
    "<정의>\n",
    "알파벳이나 단어 같은 기본 요소들을 일정한 길이를 가지는 벡터 공간에 투영하는 것이다\n",
    "특히 단어들을 벡터화하는 것을 word2vec이라고 한다.\n",
    "<방법>\n",
    "1. CBOW\n",
    "-> 주변 단어들로 부터 가운데 들어갈 단어가 나오도록 임베딩하는 방식, 구체적으로는 주변단어를 입력층으로 받고 은닉층을 하나 추가한다. 그리고 임베딩하고자 하는 단어를 출력하도록 모델을 학습시킨다. 최종적으로는 은닉층의 벡터를 임베딩 벡터로 사용한다.\n",
    "\n",
    "2. Skim-gram\n",
    "-> CBOW와 반대로, 중심단어로 부터 주변단어가 나오도록 한다. \n",
    "\n",
    "임베딩 역시 학습이 가능하다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d229ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d03d486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "chunk_len = 200\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "num_layers = 1\n",
    "embedding_size = 70\n",
    "lr = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8c29900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\r",
      "\u000b",
      "\f",
      "\n",
      "num_chars =  100\n"
     ]
    }
   ],
   "source": [
    "# import 했던 string에서 출력가능한 문자들을 다 불러옵니다. \n",
    "all_characters = string.printable\n",
    "\n",
    "# 출력가능한 문자들의 개수를 저장해놓습니다.\n",
    "n_characters = len(all_characters)\n",
    "print(all_characters)\n",
    "print('num_chars = ', n_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5fbe069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n"
     ]
    }
   ],
   "source": [
    "file = unidecode.unidecode(open('./Ch6_data/input.txt').read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b9dba1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salisbury\n",
      "Is gone to meet the king, who lately landed\n",
      "With some few private friends upon this coast.\n"
     ]
    }
   ],
   "source": [
    "# input.txt의 일부분을 chunk_len 길이만큼 발췌해 오는 함수\n",
    "def random_chunk(chunk_len):\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len \n",
    "    return file[start_index:end_index]\n",
    "\n",
    "chunk_len = 100\n",
    "print(random_chunk(chunk_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9afcae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 37, 38, 13, 14, 15])\n"
     ]
    }
   ],
   "source": [
    "# 문자열을 받았을때 이를 인덱스의 배열로 바꿔주는 함수입니다.\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long() #long type은 int형 자료형이라 생각하면 된다.\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "print(char_tensor('0BCdef'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ceca627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤한 텍스트 chunk를 불러와서 이를 입력과 목표값을 바꿔주는 함수입니다.\n",
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6e73cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNN,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, embedding_size) \n",
    "        #nn.Embedding의 input_size는 사용할 문자의 개수라고 생각하면 된다.\n",
    "        #인덱스 배열 [0,1,2]를 받으면, 3xembedding_size size로 반환한다.\n",
    "        self.rnn = nn.RNN(input_size=embedding_size,hidden_size=hidden_size, num_layers=num_layers)\n",
    "        #self.rnn = nn.GRU(input_size=embedding_size,hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        out = self.encoder(input.view(1,-1))\n",
    "        out,hidden = self.rnn(out,hidden)\n",
    "        out = self.decoder(out.view(batch_size,-1))\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, hidden_size)\n",
    "        return hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "02a1093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8adb31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size=n_characters,embedding_size = embedding_size, hidden_size=hidden_size,\n",
    "            output_size=n_characters, num_layers=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "74e696f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100]) torch.Size([2, 1, 100])\n"
     ]
    }
   ],
   "source": [
    "inp = char_tensor(\"A\").to(device)\n",
    "hidden = model.init_hidden().to(device)\n",
    "out,hidden = model(inp, hidden)\n",
    "print(out.shape, hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eb119562",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[94],\n",
      "        [12],\n",
      "        [17],\n",
      "        [18],\n",
      "        [21],\n",
      "        [13],\n",
      "        [27],\n",
      "        [14],\n",
      "        [23],\n",
      "        [94],\n",
      "        [24],\n",
      "        [27],\n",
      "        [94],\n",
      "        [17],\n",
      "        [18],\n",
      "        [28],\n",
      "        [94],\n",
      "        [32],\n",
      "        [18],\n",
      "        [15],\n",
      "        [14],\n",
      "        [68],\n",
      "        [28],\n",
      "        [94],\n",
      "        [10],\n",
      "        [21],\n",
      "        [21],\n",
      "        [18],\n",
      "        [14],\n",
      "        [28],\n",
      "        [96],\n",
      "        [55],\n",
      "        [17],\n",
      "        [18],\n",
      "        [28],\n",
      "        [94],\n",
      "        [18],\n",
      "        [28],\n",
      "        [94],\n",
      "        [29],\n",
      "        [17],\n",
      "        [14],\n",
      "        [94],\n",
      "        [13],\n",
      "        [10],\n",
      "        [34],\n",
      "        [94],\n",
      "        [32],\n",
      "        [17],\n",
      "        [14],\n",
      "        [27],\n",
      "        [14],\n",
      "        [18],\n",
      "        [23],\n",
      "        [94],\n",
      "        [44],\n",
      "        [94],\n",
      "        [32],\n",
      "        [18],\n",
      "        [28],\n",
      "        [17],\n",
      "        [68],\n",
      "        [13],\n",
      "        [94],\n",
      "        [29],\n",
      "        [24],\n",
      "        [94],\n",
      "        [15],\n",
      "        [10],\n",
      "        [21],\n",
      "        [21],\n",
      "        [96],\n",
      "        [37],\n",
      "        [34],\n",
      "        [94],\n",
      "        [29],\n",
      "        [17],\n",
      "        [14],\n",
      "        [94],\n",
      "        [15],\n",
      "        [10],\n",
      "        [21],\n",
      "        [28],\n",
      "        [14],\n",
      "        [94],\n",
      "        [15],\n",
      "        [10],\n",
      "        [18],\n",
      "        [29],\n",
      "        [17],\n",
      "        [94],\n",
      "        [24],\n",
      "        [15],\n",
      "        [94],\n",
      "        [17],\n",
      "        [18],\n",
      "        [22],\n",
      "        [94],\n",
      "        [44],\n",
      "        [94]])\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 100\n",
    "total = char_tensor(random_chunk(chunk_len))\n",
    "print(total.view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "86f1cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function & Optimizer\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "949c88cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 문자(start_str)로 시작하는 길이 200짜리 모방 글을 생성하는 코드입니다.\n",
    "def test():\n",
    "    start_str = \"b\"\n",
    "    inp = char_tensor(start_str)\n",
    "    hidden = model.init_hidden().to(device)\n",
    "    x = inp.to(device)\n",
    "\n",
    "    print(start_str,end=\"\")\n",
    "    for i in range(200):\n",
    "        output,hidden = model(x,hidden)\n",
    "        # 여기서 max값을 사용하지 않고 multinomial을 사용하는 이유는 만약 max 값만 쓰는 경우에\n",
    "        # 생성되는 텍스트가 다 the the the the the 이런식으로 나오기 때문입니다.\n",
    "        # multinomial 함수를 통해 높은 값을 가지는 문자들중에 랜덤하게 다음 글자를 뽑아내는 방식으로 자연스러운 텍스트를 생성해냅니다.\n",
    "        output_dist = output.data.view(-1).div(0.8).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = all_characters[top_i]\n",
    "        \n",
    "        print(predicted_char,end=\"\")\n",
    "\n",
    "        x = char_tensor(predicted_char).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7463ef5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([4.5736], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "j89\tX*Nt2W=qspoP$!\\z`XBn^z<])xoxcdU<9sn#\u000b",
      "?*:9=hd,w%='[i*T?).xa{_whDT1jH\"(AwEwNj\tu//5tq.QA}tsc$2:)8q3isr$86R1-?$@x(}q+~>^(_q.zfLR-g(>NL]\f",
      "QWSdSI6kS,@%L(lqa\n",
      "[G97NK}#`q\"\n",
      " zAC?[jyB^ip`!R\t4Ahp\u000b",
      "w_\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.5167], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "bbd wing the aon by letme ahe broee sauy heI be hy inE ar ere sir, polt efrsetin er vestria no sit thet nor ginl there rouwt torees dou te hour wehivos ond an ingnothin yorgy moireils, be tit rone weri\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.3652], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "befren; bere git hy mee,\n",
      "Bapein. Ksele sat en heas ciirg witis sals lienfote'd as case sorreies tiund, liten nolsll it\n",
      "Anpeard sire saneat: hind;\n",
      "Wath lore thea te stay wethe here of aold, od sat hane \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.3258], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "be l fryere, the vove is sill her orstivime sill of her thead!\n",
      "Dnd dor woet as nom heJ me poud some hed thabe hit shous and feed a cond ca harl the af youghured fyou,\n",
      "Thinl dat on ond ard ar linl sur h\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.4259], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "bllarlast by erelag, gnath speakend,\n",
      "And you has 'rake in now hat' the ence mut scefind moon dat, me are to she begreart, and soret wison, hoR that mile bohe wion\n",
      "That on fus; y\n",
      "Arrase, fere a'd the do\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1548], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "bld the of mance swat you worlill warrese lone: muld and would and youf old,\n",
      "And fecoll, hour dool: I'ald's he do when of but I Nonfing wo dorle be what of ourde uncill wiver that grom Greif le: of, bo\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0717], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "blinse in the comfant so's and lord of mor hous of hove in hef Care all prong, umark foor a blotd lowges we foobwing and mage I sand't his I hor food and upur, thou maigtege the munt to gull as that no\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0151], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "bush the me sive\n",
      "The garth Bime,\n",
      "Badt the thet my the ver, I lurk, heirmer, there foor be that thee wark gour lang, embem a vertery,\n",
      "I stour eak?\n",
      "\n",
      "CAUT:\n",
      "Whake of rayt, toos my brake have in store, enth\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0420], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "but thot that ink whack santers, as hive seice had's as more shall the sif the you my of groneds, when have sagick there sin both brought with the are praing.\n",
      "\n",
      "CRETENSSINNE:\n",
      "On tis pewing.\n",
      "\n",
      "JLI@S:\n",
      "Crei\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0793], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "brech\n",
      "Then the putis aowarn ben!\n",
      "Brother live beire.\n",
      " KENRION:\n",
      "Bo deese I to hone so star foke\n",
      "That there thess not.\n",
      "\n",
      "PAREIT:\n",
      "O we qood and is my allers end hing that him! inciol, theive pacher burne,\n",
      "\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0536], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "but the bome 't lo, nou and what vave frown and in son, has your that freak that tay on of act and thou farst blous, shat sell onous thip of her on agever thou dorm.\n",
      "\n",
      "MODUAT:\n",
      "Hery for Bord you comping \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8409], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "be! to witing sears and ang the ullition.\n",
      "\n",
      "KUCEDN TVORO:\n",
      "On to do would stould conters airst onter of tone bother to in onered mishent not be the concongast a tost our to helr, nod when contle hat song\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.5907], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "brease it nor Bairst my a prevelin; the cear thou dept my wordst best thee not it distition.\n",
      "\n",
      "ARICHARD IIUT:\n",
      "I congair my owst and ever seres- nostrer broven go go veingnen his and is mest caw prad the\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9412], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "bour stort a more of casters prom and mash my for not prove hish pelave une;\n",
      "Bowelvien it the is the hors,\n",
      "And to for thas serd he hore my from the stench that in the nave the comect you aight the my n\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7453], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "bles mars, the dook's and twe congeare,\n",
      "I with the this that more to sose\n",
      "For thee a lordere, now this air at to the ale tan shepe?\n",
      "\n",
      "SICALET:\n",
      "Thy sestet in the with the sar how, I lave the seet at as t\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1773], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "be at Cave for on his should to have his then of wild not the shall seavty, and it,\n",
      "Now with he sirsI, all to must eak now.\n",
      "Mo misher?\n",
      "\n",
      "DUKE VI:\n",
      "To my have stear tham and will the puith thy an his hear\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2567], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "brothne, the father of that to fatile of the knase.\n",
      "\n",
      "ORENENIO:\n",
      "Sis; of consed.\n",
      "\n",
      "Firnter is man the lastins spuntiunt put, michent for conguies.\n",
      "\n",
      "BUCINTIO:\n",
      "Is fartite.\n",
      "\n",
      "SEAREIS:\n",
      "Whisiluneasdled\n",
      "And of w\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9429], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "bleim in the prompent you the lit in will me rown.\n",
      "\n",
      "PEONGONTENN MORCESTEO:\n",
      "I'l on!\n",
      "\n",
      "LELLE:\n",
      "Be the not do peare store, ofe, he jeal my the make the fachars my list! decention, I words,\n",
      "Fill O'll good it\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8560], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "brestent thou he thou sartwel.\n",
      "\n",
      "LUCENRIO:\n",
      "He oth one to lords as me preentanight worte gove,\n",
      "That hase;\n",
      "What senses of dall the come Cainteren of at the foinst smudy of enence be his lentle he come,\n",
      "Bu\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1289], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "but that much all diching. for the soming tire where our my tomy boy, make hereys one be, her fords, will all I here bepore a gaich a kir off, the loves ape bodst all with their aughed, by say coman co\n",
      " ====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    total = char_tensor(random_chunk(chunk_len))\n",
    "    inp = total[:-1]\n",
    "    label = total[1:]\n",
    "    hidden = model.init_hidden().to(device)\n",
    "    \n",
    "    loss = torch.tensor([0]).type(torch.FloatTensor).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    for j in range(chunk_len-1):\n",
    "        x = inp[j].to(device)\n",
    "        y_ = label[j].unsqueeze(0).type(torch.LongTensor).to(device)\n",
    "        out,hidden = model(x, hidden)\n",
    "        loss += loss_func(out,y_)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
    "        test()\n",
    "        print(\"\\n\",\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5de44b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNN,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, embedding_size) \n",
    "        #nn.Embedding의 input_size는 사용할 문자의 개수라고 생각하면 된다.\n",
    "        #인덱스 배열 [0,1,2]를 받으면, 3xembedding_size size로 반환한다.\n",
    "        self.rnn = nn.LSTM(input_size=embedding_size,hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden, cell):\n",
    "        out = self.encoder(input.view(batch_size,-1))\n",
    "        out,(hidden,cell) = self.rnn(out,(hidden,cell))\n",
    "        out = self.decoder(out.view(batch_size,-1))\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, hidden_size)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, hidden_size)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "9380d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = RNN(input_size=n_characters,embedding_size = embedding_size, hidden_size=hidden_size,\n",
    "            output_size=n_characters, num_layers=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6164bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function & Optimizer\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a3449e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 문자(start_str)로 시작하는 길이 200짜리 모방 글을 생성하는 코드입니다.\n",
    "def test():\n",
    "    start_str = \"b\"\n",
    "    inp = char_tensor(start_str)\n",
    "    hidden, cell = lstm.init_hidden()\n",
    "    hidden = hidden.to(device)\n",
    "    cell = cell.to(device)\n",
    "    x = inp.to(device)\n",
    "\n",
    "    print(start_str,end=\"\")\n",
    "    for i in range(200):\n",
    "        output,hidden = lstm(x,hidden,cell)\n",
    "        # 여기서 max값을 사용하지 않고 multinomial을 사용하는 이유는 만약 max 값만 쓰는 경우에\n",
    "        # 생성되는 텍스트가 다 the the the the the 이런식으로 나오기 때문입니다.\n",
    "        # multinomial 함수를 통해 높은 값을 가지는 문자들중에 랜덤하게 다음 글자를 뽑아내는 방식으로 자연스러운 텍스트를 생성해냅니다.\n",
    "        output_dist = output.data.view(-1).div(0.8).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = all_characters[top_i]\n",
    "        \n",
    "        print(predicted_char,end=\"\")\n",
    "\n",
    "        x = char_tensor(predicted_char).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "cedbaf1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([4.5753], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "b\\a3rf%ZmX=_X DA7}hwi!lu`F-q-clf<O\tKE6)%,K]oD`Aa{2hL(#(1W[#6>5k)jOhK?N{-]#B8;as$&A?Cn/3QBE(.2hvWQYWd}|PK6qsq1Z\"^pc;D] #U1I Y\"+*)SMOD4WMK5C,-\\DTF\t|r`\"4Bx0<0j9m33D@S2&kpZC1!2J>\\|C9VC\\/;3!n04n5GJ16ul\n",
      "\f",
      "^k1\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.3813], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "ber yi: f thher wer in yod, hfo hinnF\n",
      "H hen oin h.dletlonn I's thaurrof ire doo ptuur ffer himef ond hher conritrtref atho ro hese boun soverimons Mon aln or, ices bhe pon hur s ror\n",
      "Nosd thns oner tre\n",
      "\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.6588], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "br adr par id wo hurle cacons anj kes ougou thod an.\n",
      "ind amor fifdee' in'linl vord hir of co! I reme sol me.\n",
      "Inf doy yas ink.\n",
      "Oo hoe won, ar souit doo oe Ir ans soonf ipir yhy thob-I thas tfoi.\n",
      "Or or a\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1209], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "by hannife fisis get I doeh sant le pon sen kinsire\n",
      "Mlome lo; whh sice don ecour I ba efote,\n",
      "Four he thet fou con Powr yound uw he the ipes, bupfer so hile,\n",
      "O yare:\n",
      "Bogeing the wo fortim worin? goigle,\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2126], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "bZ\n",
      "\n",
      "GEE:\n",
      "The corom and for mont for fost Il roc ther ding\n",
      "Oe bir ther,\n",
      "A mend souladn!\n",
      "Whe masee lost the or ind me me panrolr nollr! Wo to deane,\n",
      "Ih bot his wropl me priker we and, me ast me lirk and \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.3209], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "ble tire bufsind a may in by thou\n",
      "I and wire in thout whagd to sis reans,\n",
      "And hadger yil neper,\n",
      "Sith lere wiset lotist and.\n",
      "\n",
      "SSHOLO:\n",
      "Thour jarld and os with ice, shath\n",
      "A houvh spandlich bur mave tunce \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2034], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "bipe wenseinrs sustey the to by love coud'd ales on be to bry would let thit tand motwawe:\n",
      "In ald the'f the hectous yout, fone\n",
      "And thy bagy the wall lear it fo restresmed the hear prever the cown ses t\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0401], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "bnoage the and of thas were my that thour at we is wering the as comeint piry save; thit, as sirnind than us by sire, armend thon lenrable mpope the deaf yous our we cly, by with and the haith hour so \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9709], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "brengenter ther do Rome:\n",
      "No mer hactire, notis it I tatestren and more fantilso menen in's be and her good,\n",
      "Or insered he ward as thee hartincincipe crold a ape the bemer prang beny nom my mir sher wid\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2370], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "be, will the vepe:\n",
      "And, thag hers.\n",
      "O make but ous sonker and go love porsess, his gecest and of then for me oou gorse hoocely Thefner:\n",
      "Mut the ollant:\n",
      "Fall thour in syou at then thou undeen ther.\n",
      "\n",
      "PUVE\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0388], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "b you nonou you shous of is he the be the we to his the to dis!\n",
      "'se so the with, then me biars, banter with the the forth I so the hemest she mablegs not bucwerd, like thile is the to parsey the dane h\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7686], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "by arping and not and your do the maby!\n",
      "\n",
      "BURIENCE:\n",
      "What of and sins then now this riend, the casgrether weads,\n",
      "Whe righer; you I lootwindned:\n",
      "Io nothaagh; is to recued of I staich museting itsearniwn r\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9906], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "bate not dor hit hel seft tince your and be too this rang wrord lord unk hours\n",
      "To, we belire!\n",
      "\n",
      "HUR DORD CVIPAS:\n",
      "Buster he stare you ksile hing dring dures who sour blong and not and of he he dother us \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8082], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "ble beforon briens of bapon son of you his seal intell fird, and have to minting have alts; the stard dister and contion the are thain;\n",
      "And her, and of so wing, bardent, soss fall so my you mer upune; \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0674], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "bord not and ours choan our ame,\n",
      "Bey, with so, sit net yee wearing,\n",
      "That lie.\n",
      "Toiesing lirters be sic me comes thou cant of vast of some the sit wradsing sood word sot cown prisuces?\n",
      "\n",
      "RAMER:\n",
      "O sous for\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8772], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "better he manther the may the pare the comant hing.\n",
      "\n",
      "MLARWARD:\n",
      "Thely, have and couse deek o ment and in thel he danbit Bordth in for hount heman.\n",
      "\n",
      "BUCHASTAOLALUS:\n",
      "Nome;\n",
      "The now wirrading lork vome, jug\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0201], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "but our yends his merand.\n",
      "\n",
      "GRCKE:\n",
      "And 'te word, is do and his thee liys? brighwent in of for ind be and thou a king borson be heard\n",
      "We't in Bomis the bread, puven the do gear for agay fit will lave and\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8233], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "bard.\n",
      "\n",
      "RABD:\n",
      "I papes ir in canspaanting then with then to must more the truned's is hound lord and and I sheard had that my live theence then youlds ip me coud, not up house with and cour:\n",
      "I this cant \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8224], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "breat be wears, Mustiester him the well madand\n",
      "The staves well Had.\n",
      "\n",
      "PORION:\n",
      "Biping, nother not one, on of with not, the pring that with him onow,\n",
      "And must with wead with me and I'le arence to to by he\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9049], device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "bman misker thee all hone thee be to not his of htistenice parther hame met the wor thererished moness,\n",
      "And my you carther comfort the confef in houde net the tryses hey presters, all you, stounce lace\n",
      " ====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    total = char_tensor(random_chunk(chunk_len))\n",
    "    inp = total[:-1]\n",
    "    label = total[1:]\n",
    "    hidden, cell = lstm.init_hidden()\n",
    "    hidden = hidden.to(device)\n",
    "    cell = cell.to(device)\n",
    "    loss = torch.tensor([0]).type(torch.FloatTensor).to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    for j in range(chunk_len-1):\n",
    "        x = inp[j].to(device)\n",
    "        y_ = label[j].unsqueeze(0).type(torch.LongTensor).to(device)\n",
    "        out,hidden = lstm(x, hidden, cell)\n",
    "        loss += loss_func(out,y_)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
    "        test()\n",
    "        print(\"\\n\",\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
